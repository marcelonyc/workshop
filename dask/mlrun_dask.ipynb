{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install these modules/versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install dask==2.9.1 distributed==2.9.1 kubernetes==10.0.0 dask_kubernetes==0.10.0 kubernetes-asyncio==10.0.0 msgpack==0.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MLRUN with Dask Distributed Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a function code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that will be distributed \n",
    "def inc(x):\n",
    "    return x+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLRun context in the case of Dask will have an extra param `dask_client`\n",
    "which is initialized based on the function spec (below), and can be used to submit Dask commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hndlr(context, x=1,y=2):\n",
    "    context.logger.info('params: x={},y={}'.format(x,y))\n",
    "    print('params: x={},y={}'.format(x,y))\n",
    "    x = context.dask_client.submit(inc, x)\n",
    "    print(x)\n",
    "    print(x.result())\n",
    "    context.log_result('y', x.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code\n",
    "# marks the end of a code section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_function, mlconf, code_to_function, mount_v3io, NewTask\n",
    "mlconf.remote_host = '40.70.31.79'  # remote cluster IP/DNS for link to dask dashboard\n",
    "mlconf.dbpath = 'http://mlrun-api:8080'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function object\n",
    "dask functions can be local (local workers), or remote (use containers in the cluster),\n",
    "in the case of `remote` users can specify the number of replica (optional) or leave blank for auto-scale.\n",
    "\n",
    "We use `code_to_function()` which packs the function code into the function object/yaml (eliminate the need to update the function image), we can use `new_function()` if the function code is part of the image or can be remotely mounted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the function from the notebook code + annotations, add volumes\n",
    "dsf = code_to_function('mydask',project='lobproject', runtime='dask').apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsf.spec.image = 'daskdev/dask:2.9.1'\n",
    "dsf.spec.remote = True\n",
    "dsf.spec.replicas = 1\n",
    "dsf.spec.service_type = 'NodePort'\n",
    "dsf.spec.image_pull_policy = 'Always'\n",
    "#dsf.spec.kfp_image = 'mlrun/dask:dev2'  # select specific image for pipeline step (must have MLRun & Dask)\n",
    "#print(dsf.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the function with extra packages\n",
    "We can skip the build section if we dont add packages (instead need to specify the image e.g. `dsf.spec.image='daskdev/dask:2.9.1'`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you want to add packages to the workers\n",
    "# dsf.build_config(base_image='daskdev/dask:2.9.1', commands=['pip install pandas'])\n",
    "# dsf.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a task using our distributed dask function (cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrun = dsf.run(handler=hndlr, params={'x': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_client = kfp.Client(namespace='default-tenant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_path = '/User/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"dask_pipeline\")\n",
    "def dask_pipe(x=1,y=10):\n",
    "    # use_db option will use a function (DB) pointer instead of adding the function spec to the YAML\n",
    "    myrun = dsf.as_step(NewTask(handler=hndlr, name=\"dask_pipeline\", params={'x': x, 'y': y}), use_db=True)\n",
    "    # is the step (dask client) need v3io access u should add: .apply(mount_v3io())\n",
    "    myrun.container.set_image_pull_policy('Always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pipeline debug\n",
    "kfp.compiler.Compiler().compile(dask_pipe, 'daskpipe.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arguments={'x':4,'y':-5}\n",
    "kfp_client.create_run_from_pipeline_func(dask_pipe,\n",
    "                                 arguments, \n",
    "                                     run_name=\"DaskExamplePipeline\", \n",
    "                                     experiment_name=\"pipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlrun_ecolab]",
   "language": "python",
   "name": "conda-env-mlrun_ecolab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
